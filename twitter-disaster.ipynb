{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nltk\nnltk.data.path.append('/kaggle/working')\nnltk.download(\"all\", download_dir='/kaggle/working')\nnltk.download(\"wordnet\", download_dir='/kaggle/working')\n\nimport os\nos.environ['NLTK_DATA'] = '/kaggle/working'\n\n!mkdir /kaggle/working/corpora/wordnet\n!unzip /kaggle/working/corpora/wordnet.zip -d /kaggle/working/corpora","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_file_path = \"/kaggle/input/nlp-getting-started/train.csv\"\ntrain_data = pd.read_csv(train_file_path)\n\nprint(train_data.columns)\n\ntest_file_path = \"/kaggle/input/nlp-getting-started/test.csv\"\ntest_data = pd.read_csv(test_file_path)\n\nprint(test_data.columns)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Class responsible for handling the input data\nclass Dataset:\n    train_data = None\n    learn_data = None\n    assess_data = None\n    test_data = None\n    combined_data = None\n    \n    target_data = None\n    \n    combined_encoded_text = None\n    train_encoded_text = None\n    test_encoded_text = None\n    learn_encoded_text = None\n    assess_encoded_text = None\n        \n    # data is pandas DataFrame\n    def __init__(self, train_data, test_data, learn_ratio, assess_ratio):\n        assert learn_ratio + assess_ratio == 1, \\\n        \"The sum of learn_ratio and assess_ratio should be equal to 1\"\n        \n        self.train_data = train_data\n        self.test_data = test_data\n        \n        self.target_data = self.train_data.iloc[:, -1]\n        self.combined_data = pd.concat([self.train_data.iloc[:, :-1], self.test_data])\n        \n        encoded_documents = self.encode_documents(self.combined_data['text'])\n        self.combined_encoded_text = pd.DataFrame.sparse.from_spmatrix(encoded_documents)\n        self.train_encoded_text = self.combined_encoded_text.iloc[:len(self.train_data)]\n        self.test_encoded_text = self.combined_encoded_text.iloc[len(self.train_data):]\n        \n        learn_len = int(learn_ratio * len(self.train_data))\n        self.learn_data  = self.train_data.iloc[:learn_len]\n        self.assess_data = self.train_data.iloc[learn_len:]\n        self.learn_data_encoded_text = self.train_encoded_text[:learn_len]\n        self.assess_data_encoded_text = self.train_encoded_text[learn_len:]\n        \n    def tokenize(self, text):\n        words = re.findall(r'\\w+', text.lower())\n        lemmatizer = WordNetLemmatizer()\n        return [lemmatizer.lemmatize(word) for word in words]\n    \n    def encode_documents(self, documents):\n        vectorizer = TfidfVectorizer(tokenizer = self.tokenize)\n        return vectorizer.fit_transform(documents)\n        ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = Dataset(train_data, test_data, 0.8, 0.2)\n\nassert len(dataset.learn_data) + len(dataset.assess_data) == len(dataset.train_data), \\\n\"The size of the learn_data and assess_data should total to the size of data\"\n\nprint(\"Learn dataset len={}\".format(len(dataset.learn_data)))\nprint(\"Assess dataset len={}\".format(len(dataset.assess_data)))\nprint(\"Total dataset len={}\".format(len(dataset.train_data)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Neural Network model\n\nimport tensorflow as tf\nfrom tensorflow import keras\n\nprint(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n\ninput_size = dataset.train_encoded_text.shape[1]\nhidden_layer_size = 10\n\nmodel = keras.models.Sequential()\nmodel.add(keras.layers.Dense(hidden_layer_size, activation='swish', input_shape=(input_size,)))\nmodel.add(keras.layers.Dense(2, activation='softmax'))\nmodel.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\nmodel.fit(dataset.train_encoded_text, dataset.target_data, epochs = 50)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Download the word embeddings\n!curl -o /kaggle/working/glove.twitter.27B.zip https://downloads.cs.stanford.edu/nlp/data/glove.twitter.27B.zip","metadata":{"execution":{"iopub.status.busy":"2023-06-21T20:29:49.834512Z","iopub.execute_input":"2023-06-21T20:29:49.835017Z","iopub.status.idle":"2023-06-21T20:34:36.159166Z","shell.execute_reply.started":"2023-06-21T20:29:49.834986Z","shell.execute_reply":"2023-06-21T20:34:36.157524Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"curl: /opt/conda/lib/libcurl.so.4: no version information available (required by curl)\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100 1449M  100 1449M    0     0  5204k      0  0:04:45  0:04:44  0:00:01 5118k0:03:55 4845k  0  0:04:39  0:01:40  0:02:59 5238k04:42  0:02:47  0:01:55 5131k:04:43  0:03:23  0:01:20 5116k04:45  0:04:45 --:--:-- 5126k\n","output_type":"stream"}]},{"cell_type":"code","source":"# Unzip the downloaded word embeddings\n!unzip /kaggle/working/glove.twitter.27B.zip -d /kaggle/working\n!ls -alt /kaggle/working","metadata":{"execution":{"iopub.status.busy":"2023-06-21T20:14:48.581428Z","iopub.execute_input":"2023-06-21T20:14:48.582191Z","iopub.status.idle":"2023-06-21T20:15:34.571309Z","shell.execute_reply.started":"2023-06-21T20:14:48.582144Z","shell.execute_reply":"2023-06-21T20:15:34.569378Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Archive:  /kaggle/working/glove.twitter.27B.zip\n  inflating: /kaggle/working/glove.twitter.27B.25d.txt  \n  inflating: /kaggle/working/glove.twitter.27B.50d.txt  \n  inflating: /kaggle/working/glove.twitter.27B.100d.txt  \n  inflating: /kaggle/working/glove.twitter.27B.200d.txt  \ntotal 5242480\ndrwxr-xr-x 3 root root       4096 Jun 21 20:15 .\n-rw-r--r-- 1 root root 1520408563 Jun 21 20:12 glove.twitter.27B.zip\ndrwxr-xr-x 2 root root       4096 Jun 21 19:59 .virtual_documents\ndrwxr-xr-x 5 root root       4096 Jun 21 19:59 ..\n---------- 1 root root        263 Jun 21 19:59 __notebook_source__.ipynb\n-rw-rw-r-- 1 root root 2057590469 Aug 14  2014 glove.twitter.27B.200d.txt\n-rw-rw-r-- 1 root root 1021669379 Aug 14  2014 glove.twitter.27B.100d.txt\n-rw-rw-r-- 1 root root  510887943 Aug 14  2014 glove.twitter.27B.50d.txt\n-r--r--r-- 1 root root  257699726 Aug 14  2014 glove.twitter.27B.25d.txt\n","output_type":"stream"}]},{"cell_type":"code","source":"!head -n 1 /kaggle/working/glove.twitter.27B.100d.txt\n!tail -n 1 /kaggle/working/glove.twitter.27B.100d.txt","metadata":{"execution":{"iopub.status.busy":"2023-06-21T20:18:27.600071Z","iopub.execute_input":"2023-06-21T20:18:27.600514Z","iopub.status.idle":"2023-06-21T20:18:29.742939Z","shell.execute_reply.started":"2023-06-21T20:18:27.600477Z","shell.execute_reply":"2023-06-21T20:18:29.741574Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"<user> 0.63006 0.65177 0.25545 0.018593 0.043094 0.047194 0.23218 0.11613 0.17371 0.40487 0.022524 -0.076731 -2.2911 0.094127 0.43293 0.041801 0.063175 -0.64486 -0.43657 0.024114 -0.082989 0.21686 -0.13462 -0.22336 0.39436 -2.1724 -0.39544 0.16536 0.39438 -0.35182 -0.14996 0.10502 -0.45937 0.27729 0.8924 -0.042313 -0.009345 0.55017 0.095521 0.070504 -1.1781 0.013723 0.17742 0.74142 0.17716 0.038468 -0.31684 0.08941 0.20557 -0.34328 -0.64303 -0.878 -0.16293 -0.055925 0.33898 0.60664 -0.2774 0.33626 0.21603 -0.11051 0.0058673 -0.64757 -0.068222 -0.77414 0.13911 -0.15851 -0.61885 -0.10192 -0.47 0.19787 0.42175 -0.18458 0.080581 -0.22545 -0.065129 -0.15328 0.087726 -0.18817 -0.08371 0.21779 0.97899 0.1092 0.022705 -0.078234 0.15595 0.083105 -0.6824 0.57469 -0.19942 0.50566 -0.18277 0.37721 -0.12514 -0.42821 -0.81075 -0.39326 -0.17386 0.55096 0.64706 -0.6093\nﾟﾟﾟｵﾔｽﾐｰ -0.028777 -0.72607 -0.8277 0.34967 0.84427 0.55021 0.42523 -0.69503 0.35228 -1.2415 -0.15464 0.077556 0.94197 -0.59194 0.28619 -0.3517 1.2142 0.079015 0.96373 -0.27027 1.2487 0.27241 -0.30544 0.43956 -0.10788 0.36839 0.37969 0.99198 -1.2269 -0.17511 1.5598 0.18123 0.23259 0.67785 -0.3761 -0.15104 -0.80734 -0.47375 0.72497 -0.93847 1.1334 -1.1272 0.54616 -0.30239 -0.76562 -0.19073 -0.58541 0.14047 -0.034108 0.46274 1.1606 -0.095143 0.38196 0.77372 -0.86911 -0.16385 0.019725 0.45792 -0.31093 0.050359 0.90046 -0.86353 0.52442 0.20303 -1.421 0.011321 0.27295 -0.54272 0.076505 -0.20294 0.75033 -0.2328 0.52686 -0.056541 0.69277 -0.30045 -0.48821 -0.57779 -0.17823 0.32527 -0.62559 0.22598 -0.72499 0.62204 0.022986 -1.3035 0.88202 -0.44325 0.1876 -0.16965 0.14068 0.76996 0.076295 0.36949 0.033445 0.57745 -0.19879 -0.69694 -0.76517 -1.0901\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\n\ndef load_glove_model(file_path, size):\n    model = defaultdict(lambda: np.array([0.0 for _ in range(size)]))\n    with open(file_path) as f:\n        for line in f:\n            tokens = line.split(' ')\n            word = tokens[0]\n            embeddings = np.array([float(value) for value in tokens[1:]])\n            model[word] = embeddings\n    return model","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Read the GloVe with pandas\nimport numpy as np\n\nglove_file_path = \"/kaggle/working/glove.twitter.27B.100d.txt\"\nglove_embeddings = pd.read_csv(glove_file_path)\n\nprint(glove_embeddings)","metadata":{"execution":{"iopub.status.busy":"2023-06-21T20:20:01.536305Z","iopub.execute_input":"2023-06-21T20:20:01.536726Z","iopub.status.idle":"2023-06-21T20:20:02.034240Z","shell.execute_reply.started":"2023-06-21T20:20:01.536691Z","shell.execute_reply":"2023-06-21T20:20:02.032692Z"},"trusted":true},"execution_count":17,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)","Cell \u001b[0;32mIn[17], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      4\u001b[0m glove_file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/kaggle/working/glove.twitter.27B.100d.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 5\u001b[0m glove_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mglove_file_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(glove_embeddings)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    947\u001b[0m )\n\u001b[1;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[1;32m    610\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[0;32m--> 611\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1778\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1771\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m     (\n\u001b[1;32m   1775\u001b[0m         index,\n\u001b[1;32m   1776\u001b[0m         columns,\n\u001b[1;32m   1777\u001b[0m         col_dict,\n\u001b[0;32m-> 1778\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[1;32m   1779\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[1;32m   1780\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1782\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/c_parser_wrapper.py:230\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[0;32m--> 230\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    231\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[1;32m    232\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/_libs/parsers.pyx:808\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/_libs/parsers.pyx:866\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/_libs/parsers.pyx:852\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/_libs/parsers.pyx:1973\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n","\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 1 fields in line 5, saw 2\n"],"ename":"ParserError","evalue":"Error tokenizing data. C error: Expected 1 fields in line 5, saw 2\n","output_type":"error"}]},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense\nfrom tensorflow.keras.initializers import Constant\nimport numpy as np","metadata":{"execution":{"iopub.status.busy":"2023-06-21T20:00:54.169363Z","iopub.execute_input":"2023-06-21T20:00:54.170255Z","iopub.status.idle":"2023-06-21T20:01:03.323246Z","shell.execute_reply.started":"2023-06-21T20:00:54.170213Z","shell.execute_reply":"2023-06-21T20:01:03.322115Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"}]},{"cell_type":"code","source":"# np.array([0 for i in range(len(test_data))])\n\noutput = model.predict(dataset.test_encoded_text)\nanswer = [0 if row[0] > row[1] else 1 for row in output]\n\npredictions = pd.DataFrame({\n    'id': test_data['id'],\n    'target': answer\n})\n\npredictions.to_csv(\"submission.csv\", index = False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}