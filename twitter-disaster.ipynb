{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-06-22T23:01:17.844069Z","iopub.execute_input":"2023-06-22T23:01:17.844581Z","iopub.status.idle":"2023-06-22T23:01:17.864291Z","shell.execute_reply.started":"2023-06-22T23:01:17.844541Z","shell.execute_reply":"2023-06-22T23:01:17.863129Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"/kaggle/input/disaster-tweets-text-embeddings/test_text_embeddings.csv\n/kaggle/input/disaster-tweets-text-embeddings/train_text_embeddings.csv\n/kaggle/input/glovetwitter27b/glove.twitter.27B.200d.txt\n/kaggle/input/glovetwitter27b/glove.twitter.27B.25d.txt\n/kaggle/input/glovetwitter27b/glove.twitter.27B.50d.txt\n/kaggle/input/glovetwitter27b/glove.twitter.27B.100d.txt\n/kaggle/input/nlp-getting-started/sample_submission.csv\n/kaggle/input/nlp-getting-started/train.csv\n/kaggle/input/nlp-getting-started/test.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install openai","metadata":{"execution":{"iopub.status.busy":"2023-06-22T23:00:32.359402Z","iopub.execute_input":"2023-06-22T23:00:32.359815Z","iopub.status.idle":"2023-06-22T23:00:47.127490Z","shell.execute_reply.started":"2023-06-22T23:00:32.359770Z","shell.execute_reply":"2023-06-22T23:00:47.126362Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting openai\n  Downloading openai-0.27.8-py3-none-any.whl (73 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.6/73.6 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: requests>=2.20 in /opt/conda/lib/python3.10/site-packages (from openai) (2.28.2)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from openai) (4.64.1)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from openai) (3.8.4)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.20->openai) (2.1.1)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.20->openai) (3.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.20->openai) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.20->openai) (2023.5.7)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->openai) (23.1.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->openai) (6.0.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->openai) (4.0.2)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->openai) (1.9.1)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->openai) (1.3.3)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->openai) (1.3.1)\nInstalling collected packages: openai\nSuccessfully installed openai-0.27.8\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import nltk\nnltk.data.path.append('/kaggle/working')\nnltk.download(\"wordnet\", download_dir='/kaggle/working')\n\nimport os\nos.environ['NLTK_DATA'] = '/kaggle/working'\n\n!mkdir /kaggle/working/corpora/wordnet\n!unzip /kaggle/working/corpora/wordnet.zip -d /kaggle/working/corpora","metadata":{"execution":{"iopub.status.busy":"2023-06-22T23:01:00.900678Z","iopub.execute_input":"2023-06-22T23:01:00.901099Z","iopub.status.idle":"2023-06-22T23:01:05.789316Z","shell.execute_reply.started":"2023-06-22T23:01:00.901066Z","shell.execute_reply":"2023-06-22T23:01:05.787842Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package wordnet to /kaggle/working...\nArchive:  /kaggle/working/corpora/wordnet.zip\n  inflating: /kaggle/working/corpora/wordnet/lexnames  \n  inflating: /kaggle/working/corpora/wordnet/data.verb  \n  inflating: /kaggle/working/corpora/wordnet/index.adv  \n  inflating: /kaggle/working/corpora/wordnet/adv.exc  \n  inflating: /kaggle/working/corpora/wordnet/index.verb  \n  inflating: /kaggle/working/corpora/wordnet/cntlist.rev  \n  inflating: /kaggle/working/corpora/wordnet/data.adj  \n  inflating: /kaggle/working/corpora/wordnet/index.adj  \n  inflating: /kaggle/working/corpora/wordnet/LICENSE  \n  inflating: /kaggle/working/corpora/wordnet/citation.bib  \n  inflating: /kaggle/working/corpora/wordnet/noun.exc  \n  inflating: /kaggle/working/corpora/wordnet/verb.exc  \n  inflating: /kaggle/working/corpora/wordnet/README  \n  inflating: /kaggle/working/corpora/wordnet/index.sense  \n  inflating: /kaggle/working/corpora/wordnet/data.noun  \n  inflating: /kaggle/working/corpora/wordnet/data.adv  \n  inflating: /kaggle/working/corpora/wordnet/index.noun  \n  inflating: /kaggle/working/corpora/wordnet/adj.exc  \n","output_type":"stream"}]},{"cell_type":"code","source":"train_file_path = \"/kaggle/input/nlp-getting-started/train.csv\"\ntrain_data = pd.read_csv(train_file_path)\n\nprint(train_data.columns)\n\ntest_file_path = \"/kaggle/input/nlp-getting-started/test.csv\"\ntest_data = pd.read_csv(test_file_path)\n\nprint(test_data.columns)","metadata":{"execution":{"iopub.status.busy":"2023-06-22T23:01:21.167186Z","iopub.execute_input":"2023-06-22T23:01:21.167560Z","iopub.status.idle":"2023-06-22T23:01:21.252018Z","shell.execute_reply.started":"2023-06-22T23:01:21.167533Z","shell.execute_reply":"2023-06-22T23:01:21.250847Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Index(['id', 'keyword', 'location', 'text', 'target'], dtype='object')\nIndex(['id', 'keyword', 'location', 'text'], dtype='object')\n","output_type":"stream"}]},{"cell_type":"code","source":"import re\nimport os\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\nfrom openai.embeddings_utils import get_embedding\n\n# Class responsible for handling the input data\nclass Dataset:\n    learn_len = None\n    \n    combined_data = None\n    train_data = None\n    test_data = None\n    learn_data = None\n    assess_data = None\n    \n    target_data = None\n    \n    combined_encoded_text = None\n    train_encoded_text = None\n    test_encoded_text = None\n    learn_encoded_text = None\n    assess_encoded_text = None\n    \n    combined_sequences = None\n    train_sequences = None\n    test_sequences = None\n    learn_sequences = None\n    assess_sequences = None\n    \n    vocab_size = None\n    tokenizer = None\n    max_sequence_length = 28\n    \n    train_embeddings_read_file_path = \"/kaggle/input/disaster-tweets-text-embeddings/train_text_embeddings.csv\"\n    test_embeddings_read_file_path = \"/kaggle/input/disaster-tweets-text-embeddings/test_text_embeddings.csv\"\n    train_embeddings_file_path = \"/kaggle/working/train_text_embeddings.csv\"\n    test_embeddings_file_path = \"/kaggle/working/test_text_embeddings.csv\"\n    combined_text_embeddings = None\n    train_text_embeddings = None\n    test_text_embeddings = None\n    learn_text_embeddings = None\n    assess_text_embeddings = None\n\n    # data is pandas DataFrame\n    def __init__(self, train_data, test_data, learn_ratio, assess_ratio):\n        assert learn_ratio + assess_ratio == 1, \\\n        \"The sum of learn_ratio and assess_ratio should be equal to 1\"\n        \n        self.train_data = train_data\n        self.test_data = test_data\n        self.learn_len = int(learn_ratio * len(self.train_data))\n        self.learn_data  = self.train_data.iloc[:self.learn_len]\n        self.assess_data = self.train_data.iloc[self.learn_len:]\n        self.combined_data = pd.concat([self.train_data.iloc[:, :-1], self.test_data])\n        self.target_data = self.train_data.iloc[:, -1]\n        \n        self.tokenizer = Tokenizer()\n        self.tokenizer.fit_on_texts(self.combined_data['text'])\n        self.vocab_size = len(self.tokenizer.word_index) + 1\n\n        self.create_sequences()\n        self.create_encoded_texts()\n        self.create_text_embeddings()\n        \n    def create_sequences(self):\n        self.combined_sequences = pad_sequences(\n            self.tokenizer.texts_to_sequences(self.combined_data['text']), \n            maxlen=self.max_sequence_length, \n            padding='post')\n        self.train_sequences = pad_sequences(\n            self.tokenizer.texts_to_sequences(self.train_data['text']),\n            maxlen=self.max_sequence_length,\n            padding='post')\n        self.test_sequences = pad_sequences(\n            self.tokenizer.texts_to_sequences(self.test_data['text']),\n            maxlen=self.max_sequence_length,\n            padding='post')\n        self.learn_sequences = pad_sequences(\n            self.tokenizer.texts_to_sequences(self.learn_data['text']),\n            maxlen=self.max_sequence_length,\n            padding='post')\n        self.assess_sequences = pad_sequences(\n            self.tokenizer.texts_to_sequences(self.assess_data['text']),\n            maxlen=self.max_sequence_length,\n            padding='post')\n        \n    def create_encoded_texts(self):\n        encoded_documents = self.tfidf_encode_documents(self.combined_data['text'])\n        self.combined_encoded_text = pd.DataFrame.sparse.from_spmatrix(encoded_documents)\n        self.train_encoded_text = self.combined_encoded_text.iloc[:len(self.train_data)]\n        self.test_encoded_text = self.combined_encoded_text.iloc[len(self.train_data):]\n        self.learn_data_encoded_text = self.train_encoded_text[:self.learn_len]\n        self.assess_data_encoded_text = self.train_encoded_text[self.learn_len:]\n        \n    def create_text_embeddings(self):\n        if self.try_to_load_embeddings():\n            print(\"Successfully loaded text embeddings from local storage !\")\n            return\n        else:\n            print(\"Failed to load text embeddings from local storage ... \")\n            self.generate_embeddings()\n            self.save_embeddings()\n            \n    def lemmatize(self, text):\n        words = re.findall(r'\\w+', text.lower())\n        lemmatizer = WordNetLemmatizer()\n        return [lemmatizer.lemmatize(word) for word in words]\n    \n    def tfidf_encode_documents(self, documents):\n        vectorizer = TfidfVectorizer(tokenizer = self.lemmatize)\n        return vectorizer.fit_transform(documents)\n    \n    def try_to_load_embeddings(self):\n        if self.try_to_read_embeddings(self.train_embeddings_file_path, self.test_embeddings_file_path) or \\\n                self.try_to_read_embeddings(self.train_embeddings_read_file_path, self.test_embeddings_read_file_path):\n            self.combined_text_embeddings = pd.concat([self.train_text_embeddings, self.test_text_embeddings])\n            self.learn_text_embeddings = self.train_text_embeddings[:self.learn_len]\n            self.assess_text_embeddings = self.train_text_embeddings[self.learn_len:]\n            return True\n        return False\n    \n    def try_to_read_embeddings(self, train_file_path, test_file_path):\n        if os.path.exists(train_file_path) and os.path.exists(test_file_path):\n            self.train_text_embeddings = pd.read_csv(train_file_path)\n            self.test_text_embeddings = pd.read_csv(test_file_path)\n            return True\n        return False\n    \n    def generate_embeddings(self, model=\"text-embedding-ada-002\"):\n        print(\"Generating embeddings using OpenAI API ... \")\n        self.combined_text_embeddings = pd.DataFrame(self.combined_data.text.apply(\n            lambda x: get_embedding(x.replace(\"\\n\", \" \"), engine=model)).tolist())\n        self.train_text_embeddings = self.combined_text_embeddings.iloc[:len(self.train_data)]\n        self.test_text_embeddings = self.combined_text_embeddings.iloc[len(self.train_data):]\n        self.learn_text_embeddings = self.train_text_embeddings.iloc[:self.learn_len]\n        self.assess_text_embeddings = self.train_text_embeddings.iloc[self.learn_len:]\n        print(\"Generated embeddings using OpenAI API ... \")\n\n    def save_embeddings(self):\n        print(\"Savings embeddings to local storage ... \")\n        self.train_text_embeddings.to_csv(self.train_embeddings_file_path)\n        self.test_text_embeddings.to_csv(self.test_embeddings_file_path)\n        ","metadata":{"execution":{"iopub.status.busy":"2023-06-22T23:01:26.346315Z","iopub.execute_input":"2023-06-22T23:01:26.346721Z","iopub.status.idle":"2023-06-22T23:01:36.341674Z","shell.execute_reply.started":"2023-06-22T23:01:26.346686Z","shell.execute_reply":"2023-06-22T23:01:36.340876Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"}]},{"cell_type":"code","source":"dataset = Dataset(train_data, test_data, 0.8, 0.2)\n\nassert len(dataset.learn_data) + len(dataset.assess_data) == len(dataset.train_data), \\\n\"The size of the learn_data and assess_data should total to the size of data\"\n\nprint(\"Learn dataset len={}\".format(len(dataset.learn_data)))\nprint(\"Assess dataset len={}\".format(len(dataset.assess_data)))\nprint(\"Total dataset len={}\".format(len(dataset.train_data)))","metadata":{"execution":{"iopub.status.busy":"2023-06-22T23:01:36.343143Z","iopub.execute_input":"2023-06-22T23:01:36.343860Z","iopub.status.idle":"2023-06-22T23:01:54.919761Z","shell.execute_reply.started":"2023-06-22T23:01:36.343772Z","shell.execute_reply":"2023-06-22T23:01:54.918476Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Successfully loaded text embeddings from local storage !\nLearn dataset len=6090\nAssess dataset len=1523\nTotal dataset len=7613\n","output_type":"stream"}]},{"cell_type":"code","source":"print(dataset.combined_text_embeddings.shape)\nprint(dataset.train_text_embeddings.shape)\nprint(dataset.test_text_embeddings.shape)\nprint(dataset.learn_text_embeddings.shape)\nprint(dataset.assess_text_embeddings.shape)","metadata":{"execution":{"iopub.status.busy":"2023-06-22T23:34:38.345887Z","iopub.execute_input":"2023-06-22T23:34:38.346271Z","iopub.status.idle":"2023-06-22T23:34:38.353168Z","shell.execute_reply.started":"2023-06-22T23:34:38.346243Z","shell.execute_reply":"2023-06-22T23:34:38.351865Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"(10876, 1537)\n(7613, 1537)\n(3263, 1537)\n(6090, 1537)\n(1523, 1537)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Neural Network model\n\nimport tensorflow as tf\nfrom tensorflow import keras\n\nprint(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n\ninput_size = dataset.train_encoded_text.shape[1]\nhidden_layer_size = 10\n\ntfidf_model = keras.models.Sequential()\ntfidf_model.add(keras.layers.Dense(hidden_layer_size, activation='swish', input_shape=(input_size,)))\ntfidf_model.add(keras.layers.Dense(2, activation='softmax'))\ntfidf_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\ntfidf_model.fit(dataset.train_encoded_text, dataset.target_data, epochs = 50)","metadata":{"execution":{"iopub.status.busy":"2023-06-22T23:34:41.020683Z","iopub.execute_input":"2023-06-22T23:34:41.021885Z","iopub.status.idle":"2023-06-22T23:36:15.119220Z","shell.execute_reply.started":"2023-06-22T23:34:41.021844Z","shell.execute_reply":"2023-06-22T23:36:15.117880Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stdout","text":"Num GPUs Available:  0\nEpoch 1/50\n238/238 [==============================] - 3s 8ms/step - loss: 0.6305 - accuracy: 0.6678\nEpoch 2/50\n238/238 [==============================] - 2s 8ms/step - loss: 0.4599 - accuracy: 0.8433\nEpoch 3/50\n238/238 [==============================] - 2s 7ms/step - loss: 0.3284 - accuracy: 0.8891\nEpoch 4/50\n238/238 [==============================] - 2s 8ms/step - loss: 0.2416 - accuracy: 0.9228\nEpoch 5/50\n238/238 [==============================] - 2s 7ms/step - loss: 0.1801 - accuracy: 0.9442\nEpoch 6/50\n238/238 [==============================] - 2s 8ms/step - loss: 0.1356 - accuracy: 0.9609\nEpoch 7/50\n238/238 [==============================] - 2s 7ms/step - loss: 0.1030 - accuracy: 0.9739\nEpoch 8/50\n238/238 [==============================] - 2s 7ms/step - loss: 0.0792 - accuracy: 0.9823\nEpoch 9/50\n238/238 [==============================] - 2s 7ms/step - loss: 0.0620 - accuracy: 0.9873\nEpoch 10/50\n238/238 [==============================] - 2s 7ms/step - loss: 0.0496 - accuracy: 0.9908\nEpoch 11/50\n238/238 [==============================] - 2s 7ms/step - loss: 0.0404 - accuracy: 0.9921\nEpoch 12/50\n238/238 [==============================] - 2s 7ms/step - loss: 0.0336 - accuracy: 0.9941\nEpoch 13/50\n238/238 [==============================] - 2s 7ms/step - loss: 0.0283 - accuracy: 0.9941\nEpoch 14/50\n238/238 [==============================] - 2s 7ms/step - loss: 0.0245 - accuracy: 0.9951\nEpoch 15/50\n238/238 [==============================] - 2s 7ms/step - loss: 0.0214 - accuracy: 0.9947\nEpoch 16/50\n238/238 [==============================] - 2s 7ms/step - loss: 0.0192 - accuracy: 0.9949\nEpoch 17/50\n238/238 [==============================] - 2s 7ms/step - loss: 0.0172 - accuracy: 0.9949\nEpoch 18/50\n238/238 [==============================] - 2s 7ms/step - loss: 0.0159 - accuracy: 0.9942\nEpoch 19/50\n238/238 [==============================] - 2s 7ms/step - loss: 0.0146 - accuracy: 0.9950\nEpoch 20/50\n238/238 [==============================] - 2s 7ms/step - loss: 0.0138 - accuracy: 0.9949\nEpoch 21/50\n238/238 [==============================] - 2s 8ms/step - loss: 0.0127 - accuracy: 0.9949\nEpoch 22/50\n238/238 [==============================] - 2s 7ms/step - loss: 0.0122 - accuracy: 0.9954\nEpoch 23/50\n238/238 [==============================] - 2s 7ms/step - loss: 0.0118 - accuracy: 0.9950\nEpoch 24/50\n238/238 [==============================] - 2s 8ms/step - loss: 0.0115 - accuracy: 0.9951\nEpoch 25/50\n238/238 [==============================] - 2s 7ms/step - loss: 0.0109 - accuracy: 0.9957\nEpoch 26/50\n238/238 [==============================] - 2s 7ms/step - loss: 0.0106 - accuracy: 0.9950\nEpoch 27/50\n238/238 [==============================] - 2s 7ms/step - loss: 0.0101 - accuracy: 0.9947\nEpoch 28/50\n238/238 [==============================] - 2s 7ms/step - loss: 0.0099 - accuracy: 0.9954\nEpoch 29/50\n238/238 [==============================] - 2s 7ms/step - loss: 0.0100 - accuracy: 0.9947\nEpoch 30/50\n238/238 [==============================] - 2s 7ms/step - loss: 0.0094 - accuracy: 0.9954\nEpoch 31/50\n238/238 [==============================] - 2s 7ms/step - loss: 0.0094 - accuracy: 0.9945\nEpoch 32/50\n238/238 [==============================] - 2s 7ms/step - loss: 0.0096 - accuracy: 0.9950\nEpoch 33/50\n238/238 [==============================] - 2s 7ms/step - loss: 0.0093 - accuracy: 0.9949\nEpoch 34/50\n238/238 [==============================] - 2s 7ms/step - loss: 0.0091 - accuracy: 0.9955\nEpoch 35/50\n238/238 [==============================] - 2s 7ms/step - loss: 0.0090 - accuracy: 0.9945\nEpoch 36/50\n238/238 [==============================] - 2s 7ms/step - loss: 0.0088 - accuracy: 0.9949\nEpoch 37/50\n238/238 [==============================] - 2s 7ms/step - loss: 0.0086 - accuracy: 0.9953\nEpoch 38/50\n238/238 [==============================] - 2s 8ms/step - loss: 0.0086 - accuracy: 0.9955\nEpoch 39/50\n238/238 [==============================] - 2s 7ms/step - loss: 0.0084 - accuracy: 0.9950\nEpoch 40/50\n238/238 [==============================] - 2s 7ms/step - loss: 0.0085 - accuracy: 0.9949\nEpoch 41/50\n238/238 [==============================] - 2s 7ms/step - loss: 0.0085 - accuracy: 0.9949\nEpoch 42/50\n238/238 [==============================] - 2s 8ms/step - loss: 0.0085 - accuracy: 0.9955\nEpoch 43/50\n238/238 [==============================] - 2s 7ms/step - loss: 0.0083 - accuracy: 0.9955\nEpoch 44/50\n238/238 [==============================] - 2s 7ms/step - loss: 0.0083 - accuracy: 0.9951\nEpoch 45/50\n238/238 [==============================] - 2s 7ms/step - loss: 0.0083 - accuracy: 0.9950\nEpoch 46/50\n238/238 [==============================] - 2s 7ms/step - loss: 0.0083 - accuracy: 0.9947\nEpoch 47/50\n238/238 [==============================] - 2s 7ms/step - loss: 0.0083 - accuracy: 0.9950\nEpoch 48/50\n238/238 [==============================] - 2s 7ms/step - loss: 0.0083 - accuracy: 0.9951\nEpoch 49/50\n238/238 [==============================] - 2s 7ms/step - loss: 0.0081 - accuracy: 0.9951\nEpoch 50/50\n238/238 [==============================] - 2s 7ms/step - loss: 0.0080 - accuracy: 0.9955\n","output_type":"stream"},{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"<keras.callbacks.History at 0x7b7b4697b190>"},"metadata":{}}]},{"cell_type":"code","source":"import numpy as np\nimport collections\n\ndef load_glove_model(file_path, size, verbose = 1):\n    model = collections.defaultdict(lambda: np.array([0.0 for _ in range(size)]))\n    with open(file_path) as f:\n        for line in f:\n            tokens = line.split(' ')\n            word = tokens[0]\n            embeddings = np.array([float(value) for value in tokens[1:]])\n            model[word] = embeddings\n    if verbose >= 1:\n        print(\"Words loaded!\")\n    return model","metadata":{"execution":{"iopub.status.busy":"2023-06-22T22:28:16.475759Z","iopub.execute_input":"2023-06-22T22:28:16.478024Z","iopub.status.idle":"2023-06-22T22:28:16.488058Z","shell.execute_reply.started":"2023-06-22T22:28:16.477966Z","shell.execute_reply":"2023-06-22T22:28:16.486720Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# Read the GloVe with pandas\nimport numpy as np\n\nembedding_size = 100\nglove_file_path = \"/kaggle/input/glovetwitter27b/glove.twitter.27B.100d.txt\"\nglove_embeddings = load_glove_model(glove_file_path, embedding_size)","metadata":{"execution":{"iopub.status.busy":"2023-06-22T22:28:16.490719Z","iopub.execute_input":"2023-06-22T22:28:16.491263Z","iopub.status.idle":"2023-06-22T22:29:11.816432Z","shell.execute_reply.started":"2023-06-22T22:28:16.491216Z","shell.execute_reply":"2023-06-22T22:29:11.815113Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Words loaded!\n","output_type":"stream"}]},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense\nfrom tensorflow.keras.initializers import Constant\nimport numpy as np\n\nprint(dataset.vocab_size)\nembedding_matrix = np.zeros((dataset.vocab_size, embedding_size))\nfor word, i in dataset.tokenizer.word_index.items():\n    embedding_matrix[i] = glove_embeddings[word]\n    \nembedding_model = Sequential()\nembedding_layer = Embedding(\n    dataset.vocab_size,\n    embedding_size,\n    embeddings_initializer=Constant(embedding_matrix),\n    input_length=dataset.max_sequence_length,\n    trainable=False)\nembedding_model.add(embedding_layer)\nembedding_model.add(LSTM(100))\nembedding_model.add(Dense(2, activation='softmax'))\nembedding_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\nembedding_model.fit(dataset.train_sequences, dataset.target_data, epochs = 50)","metadata":{"execution":{"iopub.status.busy":"2023-06-22T22:29:11.818161Z","iopub.execute_input":"2023-06-22T22:29:11.818508Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"29320\nEpoch 1/50\n238/238 [==============================] - 9s 26ms/step - loss: 0.4743 - accuracy: 0.7804\nEpoch 2/50\n238/238 [==============================] - 6s 27ms/step - loss: 0.4278 - accuracy: 0.8141\nEpoch 3/50\n238/238 [==============================] - 6s 26ms/step - loss: 0.4094 - accuracy: 0.8246\nEpoch 4/50\n238/238 [==============================] - 6s 26ms/step - loss: 0.3970 - accuracy: 0.8298\nEpoch 5/50\n238/238 [==============================] - 7s 27ms/step - loss: 0.3766 - accuracy: 0.8400\nEpoch 6/50\n238/238 [==============================] - 6s 26ms/step - loss: 0.3625 - accuracy: 0.8510\nEpoch 7/50\n224/238 [===========================>..] - ETA: 0s - loss: 0.3424 - accuracy: 0.8598","output_type":"stream"}]},{"cell_type":"code","source":"# Neural Network model\n\nimport tensorflow as tf\nfrom tensorflow import keras\n\nprint(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n\nprint(dataset.test_text_embeddings)\n\ninput_size = dataset.train_text_embeddings.shape[1]\nhidden_layer_size = 10\n\nopenai_model = keras.models.Sequential()\nopenai_model.add(keras.layers.Dense(hidden_layer_size, activation='swish', input_shape=(input_size,)))\nopenai_model.add(keras.layers.Dense(2, activation='softmax'))\nopenai_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\nopenai_model.fit(dataset.train_text_embeddings, dataset.target_data, epochs = 500)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# output = np.array([0 for i in range(len(test_data))])\n\nopenai_output = openai_model.predict(dataset.test_text_embeddings)\nembedding_output = embedding_model.predict(dataset.test_sequences)\ntfidf_output = tfidf_model.predict(dataset.test_encoded_text)\n\noutput = embedding_output * tfidf_output * openai_output\nanswer = [0 if row[0] > row[1] else 1 for row in output]\n\npredictions = pd.DataFrame({\n    'id': test_data['id'],\n    'target': answer\n})\n\npredictions.to_csv(\"submission.csv\", index = False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import openai\ndef get_chatgpts_opinion(tweet, model_name=\"gpt-3.5-turbo\"):\n    chat_completion = openai.ChatCompletion.create(\n    model=model_name,\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Output a single letter, 1 if the following tweet is about a real disaster and 0 if not: \" + tweet.replace(\"\\n\", \" \")\n        }])\n    return chat_completion.choices[0].message.content","metadata":{"execution":{"iopub.status.busy":"2023-06-22T23:02:34.440967Z","iopub.execute_input":"2023-06-22T23:02:34.441345Z","iopub.status.idle":"2023-06-22T23:02:34.448391Z","shell.execute_reply.started":"2023-06-22T23:02:34.441319Z","shell.execute_reply":"2023-06-22T23:02:34.447076Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# Set up and test the OpenAI API\nimport openai\nimport time\n\nopenai.api_key = \"\"\n\nwhile len(chatgpt_output) != len(dataset.test_data):\n    try:\n        index = len(chatgpt_output)\n        text = dataset.test_data.iloc[index]['text']\n        chatgpt_output.append(1 if get_chatgpts_opinion(text) == '1' else 0)\n        if len(chatgpt_output) % 20 == 0:\n            print(\"Finished with {} tweets ...\".format(len(chatgpt_output)))\n    except:\n        print(\"Failed, will try again soon ...\")\n        print(\"Current output length: {}\".format(len(chatgpt_output)))\n        time.sleep(5)\n\nprint(len(chatgpt_output))\nprint(chatgpt_output[:10])\nprint(dataset.test_data.head(10)['text'])","metadata":{"execution":{"iopub.status.busy":"2023-06-23T00:03:16.339973Z","iopub.execute_input":"2023-06-23T00:03:16.340376Z","iopub.status.idle":"2023-06-23T00:12:21.580083Z","shell.execute_reply.started":"2023-06-23T00:03:16.340349Z","shell.execute_reply":"2023-06-23T00:12:21.578905Z"},"trusted":true},"execution_count":50,"outputs":[{"name":"stdout","text":"Finished with 2440 tweets ...\nFinished with 2460 tweets ...\nFinished with 2480 tweets ...\nFailed, will try again soon ...\nCurrent output length: 2481\nFinished with 2500 tweets ...\nFinished with 2520 tweets ...\nFinished with 2540 tweets ...\nFinished with 2560 tweets ...\nFinished with 2580 tweets ...\nFinished with 2600 tweets ...\nFinished with 2620 tweets ...\nFailed, will try again soon ...\nCurrent output length: 2633\nFinished with 2640 tweets ...\nFinished with 2660 tweets ...\nFailed, will try again soon ...\nCurrent output length: 2672\nFinished with 2680 tweets ...\nFailed, will try again soon ...\nCurrent output length: 2690\nFinished with 2700 tweets ...\nFinished with 2720 tweets ...\nFinished with 2740 tweets ...\nFinished with 2760 tweets ...\nFinished with 2780 tweets ...\nFailed, will try again soon ...\nCurrent output length: 2799\nFinished with 2800 tweets ...\nFinished with 2820 tweets ...\nFinished with 2840 tweets ...\nFinished with 2860 tweets ...\nFinished with 2880 tweets ...\nFinished with 2900 tweets ...\nFinished with 2920 tweets ...\nFinished with 2940 tweets ...\nFinished with 2960 tweets ...\nFinished with 2980 tweets ...\nFinished with 3000 tweets ...\nFinished with 3020 tweets ...\nFinished with 3040 tweets ...\nFinished with 3060 tweets ...\nFinished with 3080 tweets ...\nFinished with 3100 tweets ...\nFinished with 3120 tweets ...\nFinished with 3140 tweets ...\nFinished with 3160 tweets ...\nFinished with 3180 tweets ...\nFinished with 3200 tweets ...\nFinished with 3220 tweets ...\nFailed, will try again soon ...\nCurrent output length: 3223\nFinished with 3240 tweets ...\nFinished with 3260 tweets ...\n3263\n[1, 1, 1, 1, 1, 1, 0, 0, 0, 0]\n0                   Just happened a terrible car crash\n1    Heard about #earthquake is different cities, s...\n2    there is a forest fire at spot pond, geese are...\n3             Apocalypse lighting. #Spokane #wildfires\n4        Typhoon Soudelor kills 28 in China and Taiwan\n5                   We're shaking...It's an earthquake\n6    They'd probably still show more life than Arse...\n7                                    Hey! How are you?\n8                                     What a nice hat?\n9                                            Fuck off!\nName: text, dtype: object\n","output_type":"stream"}]},{"cell_type":"code","source":"# tfidf_output = tfidf_model.predict(dataset.test_encoded_text)\n\ncnt_correct = 0\nfor i, x in enumerate(chatgpt_output):\n    y = 0 if tfidf_output[i, 0] > tfidf_output[i, 1] else 1\n    if x == y:\n        cnt_correct += 1\n        \n# print(chatgpt_output)\n# print(tfidf_output[i, 0])\n        \nprint(cnt_correct / len(chatgpt_output))","metadata":{"execution":{"iopub.status.busy":"2023-06-23T00:13:04.564277Z","iopub.execute_input":"2023-06-23T00:13:04.564663Z","iopub.status.idle":"2023-06-23T00:13:04.576426Z","shell.execute_reply.started":"2023-06-23T00:13:04.564635Z","shell.execute_reply":"2023-06-23T00:13:04.575092Z"},"trusted":true},"execution_count":52,"outputs":[{"name":"stdout","text":"0.747471651854122\n","output_type":"stream"}]},{"cell_type":"code","source":"legacy_chatgpt_output = chatgpt_output\nprint(legacy_chatgpt_output)\nprint(len(legacy_chatgpt_output))","metadata":{"execution":{"iopub.status.busy":"2023-06-23T00:12:54.036063Z","iopub.execute_input":"2023-06-23T00:12:54.037151Z","iopub.status.idle":"2023-06-23T00:12:54.043333Z","shell.execute_reply.started":"2023-06-23T00:12:54.037113Z","shell.execute_reply":"2023-06-23T00:12:54.042124Z"},"trusted":true},"execution_count":51,"outputs":[{"name":"stdout","text":"[1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1]\n3263\n","output_type":"stream"}]},{"cell_type":"code","source":"predictions = pd.DataFrame({\n    'id': test_data['id'],\n    'target': chatgpt_output\n})\n\npredictions.to_csv(\"submission.csv\", index = False)","metadata":{"execution":{"iopub.status.busy":"2023-06-23T00:13:09.648667Z","iopub.execute_input":"2023-06-23T00:13:09.649089Z","iopub.status.idle":"2023-06-23T00:13:09.670099Z","shell.execute_reply.started":"2023-06-23T00:13:09.649059Z","shell.execute_reply":"2023-06-23T00:13:09.668817Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}