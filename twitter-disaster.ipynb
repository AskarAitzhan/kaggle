{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-06-21T21:14:56.488700Z","iopub.execute_input":"2023-06-21T21:14:56.489154Z","iopub.status.idle":"2023-06-21T21:14:56.503427Z","shell.execute_reply.started":"2023-06-21T21:14:56.489115Z","shell.execute_reply":"2023-06-21T21:14:56.502148Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"/kaggle/input/nlp-getting-started/sample_submission.csv\n/kaggle/input/nlp-getting-started/train.csv\n/kaggle/input/nlp-getting-started/test.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"import nltk\nnltk.data.path.append('/kaggle/working')\nnltk.download(\"wordnet\", download_dir='/kaggle/working')\n\nimport os\nos.environ['NLTK_DATA'] = '/kaggle/working'\n\n!mkdir /kaggle/working/corpora/wordnet\n!unzip /kaggle/working/corpora/wordnet.zip -d /kaggle/working/corpora","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_file_path = \"/kaggle/input/nlp-getting-started/train.csv\"\ntrain_data = pd.read_csv(train_file_path)\n\nprint(train_data.columns)\n\ntest_file_path = \"/kaggle/input/nlp-getting-started/test.csv\"\ntest_data = pd.read_csv(test_file_path)\n\nprint(test_data.columns)","metadata":{"execution":{"iopub.status.busy":"2023-06-21T21:28:40.081479Z","iopub.execute_input":"2023-06-21T21:28:40.081950Z","iopub.status.idle":"2023-06-21T21:28:40.160733Z","shell.execute_reply.started":"2023-06-21T21:28:40.081917Z","shell.execute_reply":"2023-06-21T21:28:40.159447Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Index(['id', 'keyword', 'location', 'text', 'target'], dtype='object')\nIndex(['id', 'keyword', 'location', 'text'], dtype='object')\n","output_type":"stream"}]},{"cell_type":"code","source":"import re\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\n# Class responsible for handling the input data\nclass Dataset:\n    train_data = None\n    learn_data = None\n    assess_data = None\n    test_data = None\n    combined_data = None\n    \n    target_data = None\n    \n    combined_encoded_text = None\n    train_encoded_text = None\n    test_encoded_text = None\n    learn_encoded_text = None\n    assess_encoded_text = None\n    \n    combined_sequences = None\n    train_sequences = None\n    test_sequences = None\n    \n    vocab_size = None\n    tokenizer = None\n    max_sequence_length = 28\n        \n    # data is pandas DataFrame\n    def __init__(self, train_data, test_data, learn_ratio, assess_ratio):\n        assert learn_ratio + assess_ratio == 1, \\\n        \"The sum of learn_ratio and assess_ratio should be equal to 1\"\n        \n        self.train_data = train_data\n        self.test_data = test_data\n        \n        self.target_data = self.train_data.iloc[:, -1]\n        self.combined_data = pd.concat([self.train_data.iloc[:, :-1], self.test_data])\n        \n        self.tokenizer = Tokenizer()\n        self.tokenizer.fit_on_texts(self.combined_data['text'])\n        self.vocab_size = len(self.tokenizer.word_index) + 1\n        self.combined_sequences = pad_sequences(\n            self.tokenizer.texts_to_sequences(self.combined_data['text']), \n            maxlen=self.max_sequence_length, \n            padding='post')\n        self.train_sequences = pad_sequences(\n            self.tokenizer.texts_to_sequences(self.train_data['text']),\n            maxlen=self.max_sequence_length,\n            padding='post')\n        self.test_sequences = pad_sequences(\n            self.tokenizer.texts_to_sequences(self.test_data['text']),\n            maxlen=self.max_sequence_length,\n            padding='post')\n\n        \n        learn_len = int(learn_ratio * len(self.train_data))\n        self.learn_data  = self.train_data.iloc[:learn_len]\n        self.assess_data = self.train_data.iloc[learn_len:]\n        \n        return\n        \n        encoded_documents = self.tfidf_encode_documents(self.combined_data['text'])\n        self.combined_encoded_text = pd.DataFrame.sparse.from_spmatrix(encoded_documents)\n        self.train_encoded_text = self.combined_encoded_text.iloc[:len(self.train_data)]\n        self.test_encoded_text = self.combined_encoded_text.iloc[len(self.train_data):]\n        self.learn_data_encoded_text = self.train_encoded_text[:learn_len]\n        self.assess_data_encoded_text = self.train_encoded_text[learn_len:]\n\n        \n    def lemmatize(self, text):\n        words = re.findall(r'\\w+', text.lower())\n        lemmatizer = WordNetLemmatizer()\n        return [lemmatizer.lemmatize(word) for word in words]\n    \n    def tfidf_encode_documents(self, documents):\n        vectorizer = TfidfVectorizer(tokenizer = self.lemmatize)\n        return vectorizer.fit_transform(documents)\n        ","metadata":{"execution":{"iopub.status.busy":"2023-06-21T21:57:02.774478Z","iopub.execute_input":"2023-06-21T21:57:02.774912Z","iopub.status.idle":"2023-06-21T21:57:02.792447Z","shell.execute_reply.started":"2023-06-21T21:57:02.774877Z","shell.execute_reply":"2023-06-21T21:57:02.790646Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"dataset = Dataset(train_data, test_data, 0.8, 0.2)\n\nassert len(dataset.learn_data) + len(dataset.assess_data) == len(dataset.train_data), \\\n\"The size of the learn_data and assess_data should total to the size of data\"\n\nprint(\"Learn dataset len={}\".format(len(dataset.learn_data)))\nprint(\"Assess dataset len={}\".format(len(dataset.assess_data)))\nprint(\"Total dataset len={}\".format(len(dataset.train_data)))","metadata":{"execution":{"iopub.status.busy":"2023-06-21T21:57:03.226020Z","iopub.execute_input":"2023-06-21T21:57:03.226659Z","iopub.status.idle":"2023-06-21T21:57:04.077328Z","shell.execute_reply.started":"2023-06-21T21:57:03.226624Z","shell.execute_reply":"2023-06-21T21:57:04.076042Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stdout","text":"Learn dataset len=6090\nAssess dataset len=1523\nTotal dataset len=7613\n","output_type":"stream"}]},{"cell_type":"code","source":"# Neural Network model\n\nimport tensorflow as tf\nfrom tensorflow import keras\n\nprint(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n\ninput_size = dataset.train_encoded_text.shape[1]\nhidden_layer_size = 10\n\nmodel = keras.models.Sequential()\nmodel.add(keras.layers.Dense(hidden_layer_size, activation='swish', input_shape=(input_size,)))\nmodel.add(keras.layers.Dense(2, activation='softmax'))\nmodel.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\nmodel.fit(dataset.train_encoded_text, dataset.target_data, epochs = 50)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Download the word embeddings\n!curl -o /kaggle/working/glove.twitter.27B.zip https://downloads.cs.stanford.edu/nlp/data/glove.twitter.27B.zip","metadata":{"execution":{"iopub.status.busy":"2023-06-21T20:29:49.834512Z","iopub.execute_input":"2023-06-21T20:29:49.835017Z","iopub.status.idle":"2023-06-21T20:34:36.159166Z","shell.execute_reply.started":"2023-06-21T20:29:49.834986Z","shell.execute_reply":"2023-06-21T20:34:36.157524Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"curl: /opt/conda/lib/libcurl.so.4: no version information available (required by curl)\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100 1449M  100 1449M    0     0  5204k      0  0:04:45  0:04:44  0:00:01 5118k0:03:55 4845k  0  0:04:39  0:01:40  0:02:59 5238k04:42  0:02:47  0:01:55 5131k:04:43  0:03:23  0:01:20 5116k04:45  0:04:45 --:--:-- 5126k\n","output_type":"stream"}]},{"cell_type":"code","source":"# Unzip the downloaded word embeddings\n!unzip /kaggle/working/glove.twitter.27B.zip -d /kaggle/working\n!ls -alt /kaggle/working","metadata":{"execution":{"iopub.status.busy":"2023-06-21T20:44:41.574776Z","iopub.execute_input":"2023-06-21T20:44:41.575241Z","iopub.status.idle":"2023-06-21T20:45:28.902826Z","shell.execute_reply.started":"2023-06-21T20:44:41.575205Z","shell.execute_reply":"2023-06-21T20:45:28.901517Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Archive:  /kaggle/working/glove.twitter.27B.zip\n  inflating: /kaggle/working/glove.twitter.27B.25d.txt  \n  inflating: /kaggle/working/glove.twitter.27B.50d.txt  \n  inflating: /kaggle/working/glove.twitter.27B.100d.txt  \n  inflating: /kaggle/working/glove.twitter.27B.200d.txt  \ntotal 5242480\ndrwxr-xr-x 3 root root       4096 Jun 21 20:45 .\n-rw-r--r-- 1 root root 1520408563 Jun 21 20:34 glove.twitter.27B.zip\ndrwxr-xr-x 2 root root       4096 Jun 21 20:29 .virtual_documents\ndrwxr-xr-x 5 root root       4096 Jun 21 20:29 ..\n---------- 1 root root        263 Jun 21 20:29 __notebook_source__.ipynb\n-rw-rw-r-- 1 root root 2057590469 Aug 14  2014 glove.twitter.27B.200d.txt\n-rw-rw-r-- 1 root root 1021669379 Aug 14  2014 glove.twitter.27B.100d.txt\n-rw-rw-r-- 1 root root  510887943 Aug 14  2014 glove.twitter.27B.50d.txt\n-r--r--r-- 1 root root  257699726 Aug 14  2014 glove.twitter.27B.25d.txt\n","output_type":"stream"}]},{"cell_type":"code","source":"!head -n 1 /kaggle/working/glove.twitter.27B.100d.txt\n!tail -n 1 /kaggle/working/glove.twitter.27B.100d.txt","metadata":{"execution":{"iopub.status.busy":"2023-06-21T21:30:01.898256Z","iopub.execute_input":"2023-06-21T21:30:01.898708Z","iopub.status.idle":"2023-06-21T21:30:04.102725Z","shell.execute_reply.started":"2023-06-21T21:30:01.898677Z","shell.execute_reply":"2023-06-21T21:30:04.101208Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"<user> 0.63006 0.65177 0.25545 0.018593 0.043094 0.047194 0.23218 0.11613 0.17371 0.40487 0.022524 -0.076731 -2.2911 0.094127 0.43293 0.041801 0.063175 -0.64486 -0.43657 0.024114 -0.082989 0.21686 -0.13462 -0.22336 0.39436 -2.1724 -0.39544 0.16536 0.39438 -0.35182 -0.14996 0.10502 -0.45937 0.27729 0.8924 -0.042313 -0.009345 0.55017 0.095521 0.070504 -1.1781 0.013723 0.17742 0.74142 0.17716 0.038468 -0.31684 0.08941 0.20557 -0.34328 -0.64303 -0.878 -0.16293 -0.055925 0.33898 0.60664 -0.2774 0.33626 0.21603 -0.11051 0.0058673 -0.64757 -0.068222 -0.77414 0.13911 -0.15851 -0.61885 -0.10192 -0.47 0.19787 0.42175 -0.18458 0.080581 -0.22545 -0.065129 -0.15328 0.087726 -0.18817 -0.08371 0.21779 0.97899 0.1092 0.022705 -0.078234 0.15595 0.083105 -0.6824 0.57469 -0.19942 0.50566 -0.18277 0.37721 -0.12514 -0.42821 -0.81075 -0.39326 -0.17386 0.55096 0.64706 -0.6093\nﾟﾟﾟｵﾔｽﾐｰ -0.028777 -0.72607 -0.8277 0.34967 0.84427 0.55021 0.42523 -0.69503 0.35228 -1.2415 -0.15464 0.077556 0.94197 -0.59194 0.28619 -0.3517 1.2142 0.079015 0.96373 -0.27027 1.2487 0.27241 -0.30544 0.43956 -0.10788 0.36839 0.37969 0.99198 -1.2269 -0.17511 1.5598 0.18123 0.23259 0.67785 -0.3761 -0.15104 -0.80734 -0.47375 0.72497 -0.93847 1.1334 -1.1272 0.54616 -0.30239 -0.76562 -0.19073 -0.58541 0.14047 -0.034108 0.46274 1.1606 -0.095143 0.38196 0.77372 -0.86911 -0.16385 0.019725 0.45792 -0.31093 0.050359 0.90046 -0.86353 0.52442 0.20303 -1.421 0.011321 0.27295 -0.54272 0.076505 -0.20294 0.75033 -0.2328 0.52686 -0.056541 0.69277 -0.30045 -0.48821 -0.57779 -0.17823 0.32527 -0.62559 0.22598 -0.72499 0.62204 0.022986 -1.3035 0.88202 -0.44325 0.1876 -0.16965 0.14068 0.76996 0.076295 0.36949 0.033445 0.57745 -0.19879 -0.69694 -0.76517 -1.0901\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nimport collections\n\ndef load_glove_model(file_path, size, verbose = 1):\n    model = collections.defaultdict(lambda: np.array([0.0 for _ in range(size)]))\n    with open(file_path) as f:\n        for line in f:\n            tokens = line.split(' ')\n            word = tokens[0]\n            embeddings = np.array([float(value) for value in tokens[1:]])\n            model[word] = embeddings\n    if verbose >= 1:\n        print(\"Words loaded!\")\n    return model","metadata":{"execution":{"iopub.status.busy":"2023-06-21T21:30:19.635077Z","iopub.execute_input":"2023-06-21T21:30:19.635527Z","iopub.status.idle":"2023-06-21T21:30:19.643628Z","shell.execute_reply.started":"2023-06-21T21:30:19.635491Z","shell.execute_reply":"2023-06-21T21:30:19.642511Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"# Read the GloVe with pandas\nimport numpy as np\n\nembedding_size = 100\nglove_file_path = \"/kaggle/working/glove.twitter.27B.100d.txt\"\nglove_embeddings = load_glove_model(glove_file_path, embedding_size)","metadata":{"execution":{"iopub.status.busy":"2023-06-21T21:45:21.671154Z","iopub.execute_input":"2023-06-21T21:45:21.671580Z","iopub.status.idle":"2023-06-21T21:46:04.046074Z","shell.execute_reply.started":"2023-06-21T21:45:21.671547Z","shell.execute_reply":"2023-06-21T21:46:04.044745Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"Words loaded!\n","output_type":"stream"}]},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense\nfrom tensorflow.keras.initializers import Constant\nimport numpy as np\n\nprint(dataset.vocab_size)\nembedding_matrix = np.zeros((dataset.vocab_size, embedding_size))\nfor word, i in dataset.tokenizer.word_index.items():\n    embedding_matrix[i] = glove_embeddings[word]\n    \nmodel = Sequential()\nembedding_layer = Embedding(\n    dataset.vocab_size,\n    embedding_size,\n    embeddings_initializer=Constant(embedding_matrix),\n    input_length=dataset.max_sequence_length,\n    trainable=False)\nmodel.add(embedding_layer)\nmodel.add(LSTM(100))\nmodel.add(Dense(2, activation='softmax'))\nmodel.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\nmodel.fit(dataset.train_sequences, dataset.target_data, epochs = 50)","metadata":{"execution":{"iopub.status.busy":"2023-06-21T21:57:30.686734Z","iopub.execute_input":"2023-06-21T21:57:30.687142Z","iopub.status.idle":"2023-06-21T22:01:55.770201Z","shell.execute_reply.started":"2023-06-21T21:57:30.687110Z","shell.execute_reply":"2023-06-21T22:01:55.769023Z"},"trusted":true},"execution_count":42,"outputs":[{"name":"stdout","text":"29320\nEpoch 1/50\n238/238 [==============================] - 8s 21ms/step - loss: 0.4799 - accuracy: 0.7846\nEpoch 2/50\n238/238 [==============================] - 5s 21ms/step - loss: 0.4364 - accuracy: 0.8107\nEpoch 3/50\n238/238 [==============================] - 5s 21ms/step - loss: 0.4087 - accuracy: 0.8208\nEpoch 4/50\n238/238 [==============================] - 5s 22ms/step - loss: 0.3939 - accuracy: 0.8302\nEpoch 5/50\n238/238 [==============================] - 5s 22ms/step - loss: 0.3787 - accuracy: 0.8400\nEpoch 6/50\n238/238 [==============================] - 5s 21ms/step - loss: 0.3610 - accuracy: 0.8487\nEpoch 7/50\n238/238 [==============================] - 5s 21ms/step - loss: 0.3429 - accuracy: 0.8616\nEpoch 8/50\n238/238 [==============================] - 5s 21ms/step - loss: 0.3246 - accuracy: 0.8714\nEpoch 9/50\n238/238 [==============================] - 5s 21ms/step - loss: 0.3037 - accuracy: 0.8823\nEpoch 10/50\n238/238 [==============================] - 5s 22ms/step - loss: 0.2780 - accuracy: 0.8926\nEpoch 11/50\n238/238 [==============================] - 5s 22ms/step - loss: 0.2517 - accuracy: 0.9045\nEpoch 12/50\n238/238 [==============================] - 5s 22ms/step - loss: 0.2368 - accuracy: 0.9109\nEpoch 13/50\n238/238 [==============================] - 5s 22ms/step - loss: 0.2042 - accuracy: 0.9236\nEpoch 14/50\n238/238 [==============================] - 5s 21ms/step - loss: 0.1780 - accuracy: 0.9348\nEpoch 15/50\n238/238 [==============================] - 5s 21ms/step - loss: 0.1635 - accuracy: 0.9376\nEpoch 16/50\n238/238 [==============================] - 5s 22ms/step - loss: 0.1480 - accuracy: 0.9479\nEpoch 17/50\n238/238 [==============================] - 5s 22ms/step - loss: 0.1401 - accuracy: 0.9489\nEpoch 18/50\n238/238 [==============================] - 5s 22ms/step - loss: 0.1271 - accuracy: 0.9547\nEpoch 19/50\n238/238 [==============================] - 5s 21ms/step - loss: 0.1135 - accuracy: 0.9601\nEpoch 20/50\n238/238 [==============================] - 5s 21ms/step - loss: 0.1163 - accuracy: 0.9564\nEpoch 21/50\n238/238 [==============================] - 5s 21ms/step - loss: 0.0971 - accuracy: 0.9631\nEpoch 22/50\n238/238 [==============================] - 5s 22ms/step - loss: 0.0960 - accuracy: 0.9636\nEpoch 23/50\n238/238 [==============================] - 5s 22ms/step - loss: 0.0765 - accuracy: 0.9711\nEpoch 24/50\n238/238 [==============================] - 5s 22ms/step - loss: 0.0786 - accuracy: 0.9706\nEpoch 25/50\n238/238 [==============================] - 5s 21ms/step - loss: 0.0740 - accuracy: 0.9723\nEpoch 26/50\n238/238 [==============================] - 5s 22ms/step - loss: 0.0677 - accuracy: 0.9725\nEpoch 27/50\n238/238 [==============================] - 5s 21ms/step - loss: 0.0732 - accuracy: 0.9704\nEpoch 28/50\n238/238 [==============================] - 5s 22ms/step - loss: 0.0604 - accuracy: 0.9750\nEpoch 29/50\n238/238 [==============================] - 5s 23ms/step - loss: 0.0540 - accuracy: 0.9781\nEpoch 30/50\n238/238 [==============================] - 5s 22ms/step - loss: 0.0711 - accuracy: 0.9719\nEpoch 31/50\n238/238 [==============================] - 5s 22ms/step - loss: 0.0634 - accuracy: 0.9732\nEpoch 32/50\n238/238 [==============================] - 5s 21ms/step - loss: 0.0615 - accuracy: 0.9758\nEpoch 33/50\n238/238 [==============================] - 5s 22ms/step - loss: 0.0676 - accuracy: 0.9729\nEpoch 34/50\n238/238 [==============================] - 5s 22ms/step - loss: 0.0678 - accuracy: 0.9736\nEpoch 35/50\n238/238 [==============================] - 5s 23ms/step - loss: 0.0504 - accuracy: 0.9782\nEpoch 36/50\n238/238 [==============================] - 5s 21ms/step - loss: 0.0435 - accuracy: 0.9803\nEpoch 37/50\n238/238 [==============================] - 5s 22ms/step - loss: 0.0447 - accuracy: 0.9796\nEpoch 38/50\n238/238 [==============================] - 5s 22ms/step - loss: 0.0613 - accuracy: 0.9756\nEpoch 39/50\n238/238 [==============================] - 5s 22ms/step - loss: 0.0623 - accuracy: 0.9752\nEpoch 40/50\n238/238 [==============================] - 5s 21ms/step - loss: 0.0497 - accuracy: 0.9795\nEpoch 41/50\n238/238 [==============================] - 5s 22ms/step - loss: 0.0423 - accuracy: 0.9807\nEpoch 42/50\n238/238 [==============================] - 5s 22ms/step - loss: 0.0483 - accuracy: 0.9771\nEpoch 43/50\n238/238 [==============================] - 5s 22ms/step - loss: 0.0473 - accuracy: 0.9783\nEpoch 44/50\n238/238 [==============================] - 5s 21ms/step - loss: 0.0447 - accuracy: 0.9794\nEpoch 45/50\n238/238 [==============================] - 5s 22ms/step - loss: 0.0562 - accuracy: 0.9768\nEpoch 46/50\n238/238 [==============================] - 5s 22ms/step - loss: 0.0573 - accuracy: 0.9750\nEpoch 47/50\n238/238 [==============================] - 5s 22ms/step - loss: 0.0572 - accuracy: 0.9770\nEpoch 48/50\n238/238 [==============================] - 5s 23ms/step - loss: 0.0432 - accuracy: 0.9808\nEpoch 49/50\n238/238 [==============================] - 5s 22ms/step - loss: 0.0484 - accuracy: 0.9775\nEpoch 50/50\n238/238 [==============================] - 5s 21ms/step - loss: 0.0513 - accuracy: 0.9783\n","output_type":"stream"},{"execution_count":42,"output_type":"execute_result","data":{"text/plain":"<keras.callbacks.History at 0x7f702c1d7430>"},"metadata":{}}]},{"cell_type":"code","source":"# np.array([0 for i in range(len(test_data))])\n\noutput = model.predict(dataset.test_sequences)\nanswer = [0 if row[0] > row[1] else 1 for row in output]\n\npredictions = pd.DataFrame({\n    'id': test_data['id'],\n    'target': answer\n})\n\npredictions.to_csv(\"submission.csv\", index = False)","metadata":{"execution":{"iopub.status.busy":"2023-06-21T22:02:10.373795Z","iopub.execute_input":"2023-06-21T22:02:10.374244Z","iopub.status.idle":"2023-06-21T22:02:12.236731Z","shell.execute_reply.started":"2023-06-21T22:02:10.374207Z","shell.execute_reply":"2023-06-21T22:02:12.235856Z"},"trusted":true},"execution_count":44,"outputs":[{"name":"stdout","text":"102/102 [==============================] - 1s 8ms/step\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}