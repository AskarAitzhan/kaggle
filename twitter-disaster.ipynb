{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-06-22T01:02:33.305584Z","iopub.execute_input":"2023-06-22T01:02:33.305936Z","iopub.status.idle":"2023-06-22T01:02:33.323307Z","shell.execute_reply.started":"2023-06-22T01:02:33.305906Z","shell.execute_reply":"2023-06-22T01:02:33.322151Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/nlp-getting-started/sample_submission.csv\n/kaggle/input/nlp-getting-started/train.csv\n/kaggle/input/nlp-getting-started/test.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"import nltk\nnltk.data.path.append('/kaggle/working')\nnltk.download(\"wordnet\", download_dir='/kaggle/working')\n\nimport os\nos.environ['NLTK_DATA'] = '/kaggle/working'\n\n!mkdir /kaggle/working/corpora/wordnet\n!unzip /kaggle/working/corpora/wordnet.zip -d /kaggle/working/corpora","metadata":{"execution":{"iopub.status.busy":"2023-06-22T01:02:33.325812Z","iopub.execute_input":"2023-06-22T01:02:33.326980Z","iopub.status.idle":"2023-06-22T01:02:39.988957Z","shell.execute_reply.started":"2023-06-22T01:02:33.326948Z","shell.execute_reply":"2023-06-22T01:02:39.987319Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package wordnet to /kaggle/working...\nArchive:  /kaggle/working/corpora/wordnet.zip\n  inflating: /kaggle/working/corpora/wordnet/lexnames  \n  inflating: /kaggle/working/corpora/wordnet/data.verb  \n  inflating: /kaggle/working/corpora/wordnet/index.adv  \n  inflating: /kaggle/working/corpora/wordnet/adv.exc  \n  inflating: /kaggle/working/corpora/wordnet/index.verb  \n  inflating: /kaggle/working/corpora/wordnet/cntlist.rev  \n  inflating: /kaggle/working/corpora/wordnet/data.adj  \n  inflating: /kaggle/working/corpora/wordnet/index.adj  \n  inflating: /kaggle/working/corpora/wordnet/LICENSE  \n  inflating: /kaggle/working/corpora/wordnet/citation.bib  \n  inflating: /kaggle/working/corpora/wordnet/noun.exc  \n  inflating: /kaggle/working/corpora/wordnet/verb.exc  \n  inflating: /kaggle/working/corpora/wordnet/README  \n  inflating: /kaggle/working/corpora/wordnet/index.sense  \n  inflating: /kaggle/working/corpora/wordnet/data.noun  \n  inflating: /kaggle/working/corpora/wordnet/data.adv  \n  inflating: /kaggle/working/corpora/wordnet/index.noun  \n  inflating: /kaggle/working/corpora/wordnet/adj.exc  \n","output_type":"stream"}]},{"cell_type":"code","source":"train_file_path = \"/kaggle/input/nlp-getting-started/train.csv\"\ntrain_data = pd.read_csv(train_file_path)\n\nprint(train_data.columns)\n\ntest_file_path = \"/kaggle/input/nlp-getting-started/test.csv\"\ntest_data = pd.read_csv(test_file_path)\n\nprint(test_data.columns)","metadata":{"execution":{"iopub.status.busy":"2023-06-22T01:02:39.990943Z","iopub.execute_input":"2023-06-22T01:02:39.993349Z","iopub.status.idle":"2023-06-22T01:02:40.090535Z","shell.execute_reply.started":"2023-06-22T01:02:39.993278Z","shell.execute_reply":"2023-06-22T01:02:40.089316Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Index(['id', 'keyword', 'location', 'text', 'target'], dtype='object')\nIndex(['id', 'keyword', 'location', 'text'], dtype='object')\n","output_type":"stream"}]},{"cell_type":"code","source":"import re\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\n# Class responsible for handling the input data\nclass Dataset:\n    train_data = None\n    learn_data = None\n    assess_data = None\n    test_data = None\n    combined_data = None\n    \n    target_data = None\n    \n    combined_encoded_text = None\n    train_encoded_text = None\n    test_encoded_text = None\n    learn_encoded_text = None\n    assess_encoded_text = None\n    \n    combined_sequences = None\n    train_sequences = None\n    test_sequences = None\n    \n    vocab_size = None\n    tokenizer = None\n    max_sequence_length = 28\n        \n    # data is pandas DataFrame\n    def __init__(self, train_data, test_data, learn_ratio, assess_ratio):\n        assert learn_ratio + assess_ratio == 1, \\\n        \"The sum of learn_ratio and assess_ratio should be equal to 1\"\n        \n        self.train_data = train_data\n        self.test_data = test_data\n        \n        self.target_data = self.train_data.iloc[:, -1]\n        self.combined_data = pd.concat([self.train_data.iloc[:, :-1], self.test_data])\n        \n        self.tokenizer = Tokenizer()\n        self.tokenizer.fit_on_texts(self.combined_data['text'])\n        self.vocab_size = len(self.tokenizer.word_index) + 1\n        self.combined_sequences = pad_sequences(\n            self.tokenizer.texts_to_sequences(self.combined_data['text']), \n            maxlen=self.max_sequence_length, \n            padding='post')\n        self.train_sequences = pad_sequences(\n            self.tokenizer.texts_to_sequences(self.train_data['text']),\n            maxlen=self.max_sequence_length,\n            padding='post')\n        self.test_sequences = pad_sequences(\n            self.tokenizer.texts_to_sequences(self.test_data['text']),\n            maxlen=self.max_sequence_length,\n            padding='post')\n\n        \n        learn_len = int(learn_ratio * len(self.train_data))\n        self.learn_data  = self.train_data.iloc[:learn_len]\n        self.assess_data = self.train_data.iloc[learn_len:]\n                \n        encoded_documents = self.tfidf_encode_documents(self.combined_data['text'])\n        self.combined_encoded_text = pd.DataFrame.sparse.from_spmatrix(encoded_documents)\n        self.train_encoded_text = self.combined_encoded_text.iloc[:len(self.train_data)]\n        self.test_encoded_text = self.combined_encoded_text.iloc[len(self.train_data):]\n        self.learn_data_encoded_text = self.train_encoded_text[:learn_len]\n        self.assess_data_encoded_text = self.train_encoded_text[learn_len:]\n\n        \n    def lemmatize(self, text):\n        words = re.findall(r'\\w+', text.lower())\n        lemmatizer = WordNetLemmatizer()\n        return [lemmatizer.lemmatize(word) for word in words]\n    \n    def tfidf_encode_documents(self, documents):\n        vectorizer = TfidfVectorizer(tokenizer = self.lemmatize)\n        return vectorizer.fit_transform(documents)\n        ","metadata":{"execution":{"iopub.status.busy":"2023-06-22T01:02:40.094591Z","iopub.execute_input":"2023-06-22T01:02:40.094937Z","iopub.status.idle":"2023-06-22T01:02:49.081491Z","shell.execute_reply.started":"2023-06-22T01:02:40.094890Z","shell.execute_reply":"2023-06-22T01:02:49.080301Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"}]},{"cell_type":"code","source":"dataset = Dataset(train_data, test_data, 0.8, 0.2)\n\nassert len(dataset.learn_data) + len(dataset.assess_data) == len(dataset.train_data), \\\n\"The size of the learn_data and assess_data should total to the size of data\"\n\nprint(\"Learn dataset len={}\".format(len(dataset.learn_data)))\nprint(\"Assess dataset len={}\".format(len(dataset.assess_data)))\nprint(\"Total dataset len={}\".format(len(dataset.train_data)))","metadata":{"execution":{"iopub.status.busy":"2023-06-22T01:02:49.084604Z","iopub.execute_input":"2023-06-22T01:02:49.085525Z","iopub.status.idle":"2023-06-22T01:02:59.238765Z","shell.execute_reply.started":"2023-06-22T01:02:49.085482Z","shell.execute_reply":"2023-06-22T01:02:59.237631Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Learn dataset len=6090\nAssess dataset len=1523\nTotal dataset len=7613\n","output_type":"stream"}]},{"cell_type":"code","source":"# Neural Network model\n\nimport tensorflow as tf\nfrom tensorflow import keras\n\nprint(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n\ninput_size = dataset.train_encoded_text.shape[1]\nhidden_layer_size = 10\n\ntfidf_model = keras.models.Sequential()\ntfidf_model.add(keras.layers.Dense(hidden_layer_size, activation='swish', input_shape=(input_size,)))\ntfidf_model.add(keras.layers.Dense(2, activation='softmax'))\ntfidf_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\ntfidf_model.fit(dataset.train_encoded_text, dataset.target_data, epochs = 50)","metadata":{"execution":{"iopub.status.busy":"2023-06-22T01:02:59.240175Z","iopub.execute_input":"2023-06-22T01:02:59.240580Z","iopub.status.idle":"2023-06-22T01:04:30.612614Z","shell.execute_reply.started":"2023-06-22T01:02:59.240537Z","shell.execute_reply":"2023-06-22T01:04:30.611402Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Num GPUs Available:  1\nEpoch 1/50\n238/238 [==============================] - 6s 6ms/step - loss: 0.6376 - accuracy: 0.6535\nEpoch 2/50\n238/238 [==============================] - 1s 6ms/step - loss: 0.4649 - accuracy: 0.8401\nEpoch 3/50\n238/238 [==============================] - 1s 6ms/step - loss: 0.3317 - accuracy: 0.8883\nEpoch 4/50\n238/238 [==============================] - 1s 6ms/step - loss: 0.2449 - accuracy: 0.9196\nEpoch 5/50\n238/238 [==============================] - 1s 6ms/step - loss: 0.1836 - accuracy: 0.9434\nEpoch 6/50\n238/238 [==============================] - 1s 5ms/step - loss: 0.1380 - accuracy: 0.9603\nEpoch 7/50\n238/238 [==============================] - 1s 5ms/step - loss: 0.1049 - accuracy: 0.9712\nEpoch 8/50\n238/238 [==============================] - 1s 6ms/step - loss: 0.0806 - accuracy: 0.9816\nEpoch 9/50\n238/238 [==============================] - 1s 6ms/step - loss: 0.0632 - accuracy: 0.9874\nEpoch 10/50\n238/238 [==============================] - 1s 5ms/step - loss: 0.0504 - accuracy: 0.9894\nEpoch 11/50\n238/238 [==============================] - 1s 5ms/step - loss: 0.0413 - accuracy: 0.9920\nEpoch 12/50\n238/238 [==============================] - 1s 6ms/step - loss: 0.0339 - accuracy: 0.9934\nEpoch 13/50\n238/238 [==============================] - 1s 5ms/step - loss: 0.0287 - accuracy: 0.9942\nEpoch 14/50\n238/238 [==============================] - 1s 5ms/step - loss: 0.0248 - accuracy: 0.9940\nEpoch 15/50\n238/238 [==============================] - 1s 5ms/step - loss: 0.0219 - accuracy: 0.9949\nEpoch 16/50\n238/238 [==============================] - 1s 5ms/step - loss: 0.0193 - accuracy: 0.9950\nEpoch 17/50\n238/238 [==============================] - 1s 6ms/step - loss: 0.0174 - accuracy: 0.9950\nEpoch 18/50\n238/238 [==============================] - 1s 5ms/step - loss: 0.0157 - accuracy: 0.9945\nEpoch 19/50\n238/238 [==============================] - 1s 5ms/step - loss: 0.0146 - accuracy: 0.9947\nEpoch 20/50\n238/238 [==============================] - 1s 5ms/step - loss: 0.0137 - accuracy: 0.9953\nEpoch 21/50\n238/238 [==============================] - 2s 7ms/step - loss: 0.0129 - accuracy: 0.9951\nEpoch 22/50\n238/238 [==============================] - 1s 6ms/step - loss: 0.0123 - accuracy: 0.9945\nEpoch 23/50\n238/238 [==============================] - 1s 6ms/step - loss: 0.0116 - accuracy: 0.9950\nEpoch 24/50\n238/238 [==============================] - 1s 6ms/step - loss: 0.0111 - accuracy: 0.9949\nEpoch 25/50\n238/238 [==============================] - 1s 5ms/step - loss: 0.0112 - accuracy: 0.9946\nEpoch 26/50\n238/238 [==============================] - 1s 5ms/step - loss: 0.0104 - accuracy: 0.9951\nEpoch 27/50\n238/238 [==============================] - 1s 5ms/step - loss: 0.0102 - accuracy: 0.9957\nEpoch 28/50\n238/238 [==============================] - 1s 5ms/step - loss: 0.0099 - accuracy: 0.9947\nEpoch 29/50\n238/238 [==============================] - 1s 5ms/step - loss: 0.0098 - accuracy: 0.9954\nEpoch 30/50\n238/238 [==============================] - 1s 5ms/step - loss: 0.0096 - accuracy: 0.9949\nEpoch 31/50\n238/238 [==============================] - 1s 5ms/step - loss: 0.0095 - accuracy: 0.9950\nEpoch 32/50\n238/238 [==============================] - 1s 5ms/step - loss: 0.0094 - accuracy: 0.9950\nEpoch 33/50\n238/238 [==============================] - 1s 5ms/step - loss: 0.0091 - accuracy: 0.9955\nEpoch 34/50\n238/238 [==============================] - 1s 5ms/step - loss: 0.0090 - accuracy: 0.9947\nEpoch 35/50\n238/238 [==============================] - 1s 5ms/step - loss: 0.0088 - accuracy: 0.9945\nEpoch 36/50\n238/238 [==============================] - 1s 5ms/step - loss: 0.0089 - accuracy: 0.9949\nEpoch 37/50\n238/238 [==============================] - 1s 5ms/step - loss: 0.0083 - accuracy: 0.9958\nEpoch 38/50\n238/238 [==============================] - 1s 5ms/step - loss: 0.0085 - accuracy: 0.9955\nEpoch 39/50\n238/238 [==============================] - 1s 5ms/step - loss: 0.0086 - accuracy: 0.9959\nEpoch 40/50\n238/238 [==============================] - 1s 6ms/step - loss: 0.0083 - accuracy: 0.9955\nEpoch 41/50\n238/238 [==============================] - 1s 5ms/step - loss: 0.0083 - accuracy: 0.9953\nEpoch 42/50\n238/238 [==============================] - 1s 5ms/step - loss: 0.0084 - accuracy: 0.9947\nEpoch 43/50\n238/238 [==============================] - 1s 5ms/step - loss: 0.0082 - accuracy: 0.9954\nEpoch 44/50\n238/238 [==============================] - 1s 5ms/step - loss: 0.0084 - accuracy: 0.9951\nEpoch 45/50\n238/238 [==============================] - 2s 7ms/step - loss: 0.0083 - accuracy: 0.9954\nEpoch 46/50\n238/238 [==============================] - 2s 7ms/step - loss: 0.0081 - accuracy: 0.9959\nEpoch 47/50\n238/238 [==============================] - 1s 6ms/step - loss: 0.0077 - accuracy: 0.9953\nEpoch 48/50\n238/238 [==============================] - 1s 5ms/step - loss: 0.0083 - accuracy: 0.9954\nEpoch 49/50\n238/238 [==============================] - 1s 5ms/step - loss: 0.0083 - accuracy: 0.9955\nEpoch 50/50\n238/238 [==============================] - 1s 5ms/step - loss: 0.0083 - accuracy: 0.9953\n","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"<keras.callbacks.History at 0x79ee19196fb0>"},"metadata":{}}]},{"cell_type":"code","source":"# Download the word embeddings\n!curl -o /kaggle/working/glove.twitter.27B.zip https://downloads.cs.stanford.edu/nlp/data/glove.twitter.27B.zip","metadata":{"execution":{"iopub.status.busy":"2023-06-22T01:04:30.615606Z","iopub.execute_input":"2023-06-22T01:04:30.615908Z","iopub.status.idle":"2023-06-22T01:09:16.117678Z","shell.execute_reply.started":"2023-06-22T01:04:30.615881Z","shell.execute_reply":"2023-06-22T01:09:16.116444Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"curl: /opt/conda/lib/libcurl.so.4: no version information available (required by curl)\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100 1449M  100 1449M    0     0  5219k      0  0:04:44  0:04:44 --:--:-- 5133k0 6511k    0     0  6832k      0  0:03:37 --:--:--  0:03:37 6832k  42  622M    0     0  5358k      0  0:04:37  0:01:58  0:02:39 5123k  0     0  5286k      0  0:04:40  0:02:49  0:01:51 5120k\n","output_type":"stream"}]},{"cell_type":"code","source":"# Unzip the downloaded word embeddings\n!unzip /kaggle/working/glove.twitter.27B.zip -d /kaggle/working\n!ls -alt /kaggle/working","metadata":{"execution":{"iopub.status.busy":"2023-06-22T01:09:16.120034Z","iopub.execute_input":"2023-06-22T01:09:16.120490Z","iopub.status.idle":"2023-06-22T01:09:57.333258Z","shell.execute_reply.started":"2023-06-22T01:09:16.120444Z","shell.execute_reply":"2023-06-22T01:09:57.332047Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Archive:  /kaggle/working/glove.twitter.27B.zip\n  inflating: /kaggle/working/glove.twitter.27B.25d.txt  \n  inflating: /kaggle/working/glove.twitter.27B.50d.txt  \n  inflating: /kaggle/working/glove.twitter.27B.100d.txt  \n  inflating: /kaggle/working/glove.twitter.27B.200d.txt  \ntotal 5242484\ndrwxr-xr-x 4 root root       4096 Jun 22 01:09 .\n-rw-r--r-- 1 root root 1520408563 Jun 22 01:09 glove.twitter.27B.zip\ndrwxr-xr-x 3 root root       4096 Jun 22 01:02 corpora\ndrwxr-xr-x 2 root root       4096 Jun 22 01:02 .virtual_documents\ndrwxr-xr-x 5 root root       4096 Jun 22 01:02 ..\n---------- 1 root root        263 Jun 22 01:02 __notebook_source__.ipynb\n-rw-rw-r-- 1 root root 2057590469 Aug 14  2014 glove.twitter.27B.200d.txt\n-rw-rw-r-- 1 root root 1021669379 Aug 14  2014 glove.twitter.27B.100d.txt\n-rw-rw-r-- 1 root root  510887943 Aug 14  2014 glove.twitter.27B.50d.txt\n-r--r--r-- 1 root root  257699726 Aug 14  2014 glove.twitter.27B.25d.txt\n","output_type":"stream"}]},{"cell_type":"code","source":"!head -n 1 /kaggle/working/glove.twitter.27B.100d.txt\n!tail -n 1 /kaggle/working/glove.twitter.27B.100d.txt","metadata":{"execution":{"iopub.status.busy":"2023-06-22T01:09:57.335539Z","iopub.execute_input":"2023-06-22T01:09:57.336328Z","iopub.status.idle":"2023-06-22T01:09:59.490649Z","shell.execute_reply.started":"2023-06-22T01:09:57.336284Z","shell.execute_reply":"2023-06-22T01:09:59.489031Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"<user> 0.63006 0.65177 0.25545 0.018593 0.043094 0.047194 0.23218 0.11613 0.17371 0.40487 0.022524 -0.076731 -2.2911 0.094127 0.43293 0.041801 0.063175 -0.64486 -0.43657 0.024114 -0.082989 0.21686 -0.13462 -0.22336 0.39436 -2.1724 -0.39544 0.16536 0.39438 -0.35182 -0.14996 0.10502 -0.45937 0.27729 0.8924 -0.042313 -0.009345 0.55017 0.095521 0.070504 -1.1781 0.013723 0.17742 0.74142 0.17716 0.038468 -0.31684 0.08941 0.20557 -0.34328 -0.64303 -0.878 -0.16293 -0.055925 0.33898 0.60664 -0.2774 0.33626 0.21603 -0.11051 0.0058673 -0.64757 -0.068222 -0.77414 0.13911 -0.15851 -0.61885 -0.10192 -0.47 0.19787 0.42175 -0.18458 0.080581 -0.22545 -0.065129 -0.15328 0.087726 -0.18817 -0.08371 0.21779 0.97899 0.1092 0.022705 -0.078234 0.15595 0.083105 -0.6824 0.57469 -0.19942 0.50566 -0.18277 0.37721 -0.12514 -0.42821 -0.81075 -0.39326 -0.17386 0.55096 0.64706 -0.6093\nﾟﾟﾟｵﾔｽﾐｰ -0.028777 -0.72607 -0.8277 0.34967 0.84427 0.55021 0.42523 -0.69503 0.35228 -1.2415 -0.15464 0.077556 0.94197 -0.59194 0.28619 -0.3517 1.2142 0.079015 0.96373 -0.27027 1.2487 0.27241 -0.30544 0.43956 -0.10788 0.36839 0.37969 0.99198 -1.2269 -0.17511 1.5598 0.18123 0.23259 0.67785 -0.3761 -0.15104 -0.80734 -0.47375 0.72497 -0.93847 1.1334 -1.1272 0.54616 -0.30239 -0.76562 -0.19073 -0.58541 0.14047 -0.034108 0.46274 1.1606 -0.095143 0.38196 0.77372 -0.86911 -0.16385 0.019725 0.45792 -0.31093 0.050359 0.90046 -0.86353 0.52442 0.20303 -1.421 0.011321 0.27295 -0.54272 0.076505 -0.20294 0.75033 -0.2328 0.52686 -0.056541 0.69277 -0.30045 -0.48821 -0.57779 -0.17823 0.32527 -0.62559 0.22598 -0.72499 0.62204 0.022986 -1.3035 0.88202 -0.44325 0.1876 -0.16965 0.14068 0.76996 0.076295 0.36949 0.033445 0.57745 -0.19879 -0.69694 -0.76517 -1.0901\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nimport collections\n\ndef load_glove_model(file_path, size, verbose = 1):\n    model = collections.defaultdict(lambda: np.array([0.0 for _ in range(size)]))\n    with open(file_path) as f:\n        for line in f:\n            tokens = line.split(' ')\n            word = tokens[0]\n            embeddings = np.array([float(value) for value in tokens[1:]])\n            model[word] = embeddings\n    if verbose >= 1:\n        print(\"Words loaded!\")\n    return model","metadata":{"execution":{"iopub.status.busy":"2023-06-22T01:09:59.493535Z","iopub.execute_input":"2023-06-22T01:09:59.494528Z","iopub.status.idle":"2023-06-22T01:09:59.521622Z","shell.execute_reply.started":"2023-06-22T01:09:59.494469Z","shell.execute_reply":"2023-06-22T01:09:59.520471Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# Read the GloVe with pandas\nimport numpy as np\n\nembedding_size = 100\nglove_file_path = \"/kaggle/working/glove.twitter.27B.100d.txt\"\nglove_embeddings = load_glove_model(glove_file_path, embedding_size)","metadata":{"execution":{"iopub.status.busy":"2023-06-22T01:09:59.525699Z","iopub.execute_input":"2023-06-22T01:09:59.526419Z","iopub.status.idle":"2023-06-22T01:10:43.855413Z","shell.execute_reply.started":"2023-06-22T01:09:59.526381Z","shell.execute_reply":"2023-06-22T01:10:43.854442Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Words loaded!\n","output_type":"stream"}]},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense\nfrom tensorflow.keras.initializers import Constant\nimport numpy as np\n\nprint(dataset.vocab_size)\nembedding_matrix = np.zeros((dataset.vocab_size, embedding_size))\nfor word, i in dataset.tokenizer.word_index.items():\n    embedding_matrix[i] = glove_embeddings[word]\n    \nembedding_model = Sequential()\nembedding_layer = Embedding(\n    dataset.vocab_size,\n    embedding_size,\n    embeddings_initializer=Constant(embedding_matrix),\n    input_length=dataset.max_sequence_length,\n    trainable=False)\nembedding_model.add(embedding_layer)\nembedding_model.add(LSTM(100))\nembedding_model.add(Dense(2, activation='softmax'))\nembedding_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\nembedding_model.fit(dataset.train_sequences, dataset.target_data, epochs = 100)","metadata":{"execution":{"iopub.status.busy":"2023-06-22T01:35:05.355597Z","iopub.execute_input":"2023-06-22T01:35:05.355963Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"29320\nEpoch 1/100\n238/238 [==============================] - 4s 7ms/step - loss: 0.4875 - accuracy: 0.7738\nEpoch 2/100\n238/238 [==============================] - 2s 9ms/step - loss: 0.4267 - accuracy: 0.8123\nEpoch 3/100\n238/238 [==============================] - 2s 7ms/step - loss: 0.4083 - accuracy: 0.8224\nEpoch 4/100\n238/238 [==============================] - 2s 7ms/step - loss: 0.3899 - accuracy: 0.8324\nEpoch 5/100\n238/238 [==============================] - 1s 6ms/step - loss: 0.3748 - accuracy: 0.8399\nEpoch 6/100\n238/238 [==============================] - 2s 6ms/step - loss: 0.3555 - accuracy: 0.8512\nEpoch 7/100\n238/238 [==============================] - 2s 6ms/step - loss: 0.3331 - accuracy: 0.8617\nEpoch 8/100\n238/238 [==============================] - 2s 7ms/step - loss: 0.3152 - accuracy: 0.8743\nEpoch 9/100\n238/238 [==============================] - 2s 7ms/step - loss: 0.2914 - accuracy: 0.8828\nEpoch 10/100\n238/238 [==============================] - 2s 8ms/step - loss: 0.2594 - accuracy: 0.9024\nEpoch 11/100\n238/238 [==============================] - 2s 7ms/step - loss: 0.2309 - accuracy: 0.9142\nEpoch 12/100\n238/238 [==============================] - 2s 7ms/step - loss: 0.2128 - accuracy: 0.9233\nEpoch 13/100\n238/238 [==============================] - 2s 8ms/step - loss: 0.1900 - accuracy: 0.9287\nEpoch 14/100\n238/238 [==============================] - 2s 8ms/step - loss: 0.1655 - accuracy: 0.9409\nEpoch 15/100\n238/238 [==============================] - 2s 7ms/step - loss: 0.1495 - accuracy: 0.9472\nEpoch 16/100\n238/238 [==============================] - 2s 7ms/step - loss: 0.1331 - accuracy: 0.9514\nEpoch 17/100\n238/238 [==============================] - 2s 7ms/step - loss: 0.1246 - accuracy: 0.9547\nEpoch 18/100\n238/238 [==============================] - 2s 7ms/step - loss: 0.1138 - accuracy: 0.9603\nEpoch 19/100\n238/238 [==============================] - 2s 7ms/step - loss: 0.1084 - accuracy: 0.9616\nEpoch 20/100\n238/238 [==============================] - 1s 6ms/step - loss: 0.0981 - accuracy: 0.9648\nEpoch 21/100\n238/238 [==============================] - 1s 6ms/step - loss: 0.0906 - accuracy: 0.9662\nEpoch 22/100\n238/238 [==============================] - 1s 6ms/step - loss: 0.0879 - accuracy: 0.9674\nEpoch 23/100\n238/238 [==============================] - 1s 6ms/step - loss: 0.0787 - accuracy: 0.9711\nEpoch 24/100\n238/238 [==============================] - 1s 6ms/step - loss: 0.0770 - accuracy: 0.9698\nEpoch 25/100\n238/238 [==============================] - 1s 6ms/step - loss: 0.0774 - accuracy: 0.9693\nEpoch 26/100\n238/238 [==============================] - 1s 6ms/step - loss: 0.0686 - accuracy: 0.9746\nEpoch 27/100\n238/238 [==============================] - 1s 6ms/step - loss: 0.0577 - accuracy: 0.9774\nEpoch 28/100\n238/238 [==============================] - 1s 6ms/step - loss: 0.0536 - accuracy: 0.9777\nEpoch 29/100\n238/238 [==============================] - 1s 6ms/step - loss: 0.0923 - accuracy: 0.9652\nEpoch 30/100\n238/238 [==============================] - 1s 6ms/step - loss: 0.0738 - accuracy: 0.9711\nEpoch 31/100\n238/238 [==============================] - 1s 5ms/step - loss: 0.0609 - accuracy: 0.9753\nEpoch 32/100\n238/238 [==============================] - 1s 5ms/step - loss: 0.0549 - accuracy: 0.9773\nEpoch 33/100\n238/238 [==============================] - 1s 5ms/step - loss: 0.0641 - accuracy: 0.9753\nEpoch 34/100\n238/238 [==============================] - 1s 5ms/step - loss: 0.0603 - accuracy: 0.9760\nEpoch 35/100\n238/238 [==============================] - 2s 7ms/step - loss: 0.0452 - accuracy: 0.9789\nEpoch 36/100\n238/238 [==============================] - 1s 5ms/step - loss: 0.0437 - accuracy: 0.9796\nEpoch 37/100\n238/238 [==============================] - 1s 6ms/step - loss: 0.0467 - accuracy: 0.9807\nEpoch 38/100\n238/238 [==============================] - 1s 5ms/step - loss: 0.0649 - accuracy: 0.9744\nEpoch 39/100\n238/238 [==============================] - 1s 5ms/step - loss: 0.0523 - accuracy: 0.9786\nEpoch 40/100\n238/238 [==============================] - 1s 5ms/step - loss: 0.0457 - accuracy: 0.9798\nEpoch 41/100\n238/238 [==============================] - 1s 5ms/step - loss: 0.0485 - accuracy: 0.9790\nEpoch 42/100\n238/238 [==============================] - 1s 5ms/step - loss: 0.0628 - accuracy: 0.9724\nEpoch 43/100\n207/238 [=========================>....] - ETA: 0s - loss: 0.0509 - accuracy: 0.9796","output_type":"stream"}]},{"cell_type":"code","source":"# np.array([0 for i in range(len(test_data))])\n\nembedding_output = embedding_model.predict(dataset.test_sequences)\ntfidf_output = tfidf_model.predict(dataset.test_encoded_text)\n\noutput = embedding_output * tfidf_output\nanswer = [0 if row[0] > row[1] else 1 for row in output]\n\npredictions = pd.DataFrame({\n    'id': test_data['id'],\n    'target': answer\n})\n\npredictions.to_csv(\"submission.csv\", index = False)","metadata":{"execution":{"iopub.status.busy":"2023-06-22T01:33:50.408858Z","iopub.execute_input":"2023-06-22T01:33:50.409557Z","iopub.status.idle":"2023-06-22T01:33:55.331907Z","shell.execute_reply.started":"2023-06-22T01:33:50.409514Z","shell.execute_reply":"2023-06-22T01:33:55.330866Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"102/102 [==============================] - 1s 3ms/step\n102/102 [==============================] - 1s 6ms/step\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}